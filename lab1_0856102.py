# -*- coding: utf-8 -*-
"""lab1-0856102.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1a8xItVSgdnbPSzntDGn3kYWYQhL-N8AT

#Recommend Similar News Articles
This notebook demonstrates how to use bag-of-word vectors and cosine similarity for news article recommendation.
"""

import re
import math
import pandas as pd
import numpy as np
from collections import Counter
from collections import defaultdict

"""### Get Stopwords"""

from urllib.request import urlopen
textpage = urlopen("https://raw.githubusercontent.com/bshmueli/108-nlp/master/stopwords.txt")
stopwords = list()
def zero():
  return 0
dfx = defaultdict(zero)
for line in textpage:
  decoded_line = line.decode("utf-8")
  #print(decoded_line)
  stopwords.append(decoded_line.strip("\n"))

"""#Fetching the Corpus
`get_corpus()` reads the CSV file, and then return a list of the news headlines
"""

def get_corpus():
  df = pd.read_csv('https://raw.githubusercontent.com/bshmueli/108-nlp/master/reuters.csv') # https://bit.ly/nlp-reuters
  print("Dataset columns", df.columns)
  print("Dataset size", len(df))
  global data_size
  data_size = len(df)
  global title
  title = df.title.to_list()
  corpus = df.content.to_list()
  return corpus

def tokenize(document):
  words = re.split('\W+', document)
  #words = document.split(' ')
  return [word.lower() if word.lower() not in stopwords else do_nothing() for word in words]
def do_nothing():
  pass

"""#Computing word frequencies
`get_vocab(corpus)` computes the word frequencies in a given corpus. It returns a list of 2-tuples. Each tuple contains the token and its frequency.
"""

def get_vocab(corpus):
  vocabulary = Counter()
  for document in corpus:
    tokens = tokenize(document)
    for token in set(tokens):
      dfx[token] += 1
    vocabulary.update(tokens)
  #print (dfx)
  del vocabulary[None]
  del vocabulary['u']
  del vocabulary['']
  return vocabulary

"""#Compute BoW (Bag-of-Words) Vector
`doc_to_vec(doc, vocab)` returns a bag-of-words vector for document `doc`, corresponding to the presence of a word in `vocab`
"""

#def doc2vec(doc):
#  words = tokenize(doc)
#  return [1 if token in words else 0 for token, freq in vocab]

# use TFIDF instead
def doc2vec(doc):
  tfidf = np.zeros(len(vocab))
  words = tokenize(doc)
  count = defaultdict(zero)
  idx = 0
  for token, freq in vocab:
    if token in words:
      for word in words:
        if word == token:
          count[token]+=1
      tfidf[idx] = count[token]/len(words) * math.log10(data_size/dfx[token])
    else:
      tfidf[idx] = 0
    idx += 1
  return tfidf

"""Compute the Bag-of-Words vector for each document

Cosine similarity between two numerical vectors
"""

def cosine_similarity(vec_a, vec_b):
  assert len(vec_a) == len(vec_b)
  if sum(vec_a) == 0 or sum(vec_b) == 0:
    return 0 # hack
  a_b = sum(i[0] * i[1] for i in zip(vec_a, vec_b))
  a_2 = sum([i*i for i in vec_a])
  b_2 = sum([i*i for i in vec_b])
  return a_b/(math.sqrt(a_2) * math.sqrt(b_2))

def doc_similarity(doc_a, doc_b):
  return cosine_similarity(doc2vec(doc_a), doc2vec(doc_b))

"""# Find Similar Documents
Find and print the $k$ most similar titles to a given title
"""

def k_similar(seed_id, k):
  seed_doc = corpus[seed_id]
  title_doc = title[seed_id]
  print('> "{}"'.format(title_doc))
  similarities = [doc_similarity(seed_doc, doc) for id, doc in enumerate(corpus)]
  top_indices = sorted(range(len(similarities)), key=lambda i: similarities[i])[-k:] # https://stackoverflow.com/questions/13070461/get-indices-of-the-top-n-values-of-a-list
  nearest = [[title[id], similarities[id]] for id in top_indices]
  print()
  for story in reversed(nearest):
    print('* "{}" ({})'.format(story[0], story[1]))

"""# Test our program"""

corpus = get_corpus()
vocab = get_vocab(corpus).most_common(1000)
k_similar(102, 5)