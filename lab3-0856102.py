# -*- coding: utf-8 -*-
"""lab3-0856102.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/167G6ancz1fOS2Yetl56KvzdA6HFJ4xHR
"""

import re, math
from collections import Counter, defaultdict
import pandas as pd
import nltk
import numpy as np
from nltk.tokenize import TweetTokenizer

corpus = pd.read_json('https://raw.githubusercontent.com/bshmueli/108-nlp/master/tweets_train.txt', lines=True)
test_dataset = pd.read_json('https://raw.githubusercontent.com/bshmueli/108-nlp/master/tweets_test.txt', lines=True)
data_clean = []
data = []
test_clean = []
test = []
tknzr = TweetTokenizer()
for document in corpus['text']:
  sentence = tknzr.tokenize(document.lower())
  data_clean.extend(sentence)
  sentence = ['<s>'] + sentence + ['</s>']
  #print(sentence)
  data.extend(sentence)
for document in test_dataset['text']:
  sentence = tknzr.tokenize(document.lower())
  test_clean.extend(sentence)
  sentence = ['<s>'] + sentence + ['</s>']
  #print(sentence)
  test.extend(sentence)
#corpus = [token for token in re.split('\W+', corpus.lower()) if token != '']
#corpus = ['<s>'] + corpus + ['</s>']
#print(corpus)
corpus_bigrams = list(nltk.bigrams(data))
test_bigrams = list(nltk.bigrams(test))
#corpus_bigrams

#sentence = 'What is the question?'
#sentence = [token for token in re.split('\W+', sentence.lower()) if token != '']
#sentence = ['<s>'] + sentence + ['</s>']
#print(sentence)
#sent_bigrams = list(nltk.bigrams(sentence))
#sent_bigrams

vocab = Counter(data_clean)
vocab = [token if freq >= 3 else '<UNK>' for token, freq in vocab.most_common()]
#vocab

counts = defaultdict(lambda: defaultdict(lambda: 0))
for w1, w2 in nltk.bigrams(data):
  counts[w1][w2] += 1

#counts

# probabilities
probabilities = [(1 + counts[w1][w2])/(len(vocab) + sum(counts[w1].values())) for w1, w2 in corpus_bigrams]
test_prob = [(1 + counts[w1][w2])/(len(vocab) + sum(counts[w1].values())) for w1, w2 in test_bigrams]

#probabilities

print("Part1: ")
N = len(corpus_bigrams)
sum0 = 0
for probability in probabilities:
  sum0 += math.pow(np.prod(probability), -1/N)
print(f"The average perplexity for train data tweet is: {sum0/N}")

N_t = len(test_bigrams)
sum1 = 0
for prob in test_prob:
  sum1 += math.pow(np.prod(prob), -1/N_t)
print(f"The average perplexity for test data tweet is: {sum1/N_t}")

print("Part2: ")
reverse_counts = defaultdict(lambda: defaultdict(lambda: 0))
for w1, w2 in nltk.bigrams(data):
  reverse_counts[w2][w1] += 1
rev_probabilities = [(1 + reverse_counts[w2][w1])/(len(vocab) + sum(reverse_counts[w2].values())) for w1, w2 in corpus_bigrams]
rev_test_prob = [(1 + reverse_counts[w2][w1])/(len(vocab) + sum(reverse_counts[w2].values())) for w1, w2 in test_bigrams]
lamb = 0
bf_prob = []
train_prob = []
sum3 = 0
lowest = 10000
best_lamb = 10

for i in range(21):
  sum3 = 0
  bf_prob = []
  for j in range(N_t-1):
    bf_prob.append( lamb * test_prob[j+1] + (1 - lamb) * rev_test_prob[j] )
  for prob in bf_prob:
    sum3 += math.pow(np.prod(prob), -1/N_t)
  if sum3/N_t <= lowest:
    lowest = sum3/N_t
    best_lamb = lamb
  lamb += 0.05
print(f"best lambda is {best_lamb}")
print(f"Average perplexity for test data tweet is {lowest}")
sum4 = 0
for i in range(N-1):
  train_prob.append(best_lamb * probabilities[i+1] + (1-best_lamb) * rev_probabilities[i])
for prob in train_prob:
  sum4 += math.pow(np.prod(prob), -1/N)
print(f"Average perplexity for train data tweet is {sum4/N}")