# -*- coding: utf-8 -*-
"""lab3-0856102.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/167G6ancz1fOS2Yetl56KvzdA6HFJ4xHR
"""

import re, math
from collections import Counter, defaultdict
import pandas as pd
import nltk
import numpy as np
from nltk.tokenize import TweetTokenizer

corpus = pd.read_json('https://raw.githubusercontent.com/bshmueli/108-nlp/master/tweets_train.txt', lines=True)
test_dataset = pd.read_json('https://raw.githubusercontent.com/bshmueli/108-nlp/master/tweets_test.txt', lines=True)
data_clean = []
data = []
test_clean = []
test = []
tknzr = TweetTokenizer()
corpus_bigrams = []
test_bigrams = []

for document in corpus['text']:
  sentence = tknzr.tokenize(document.lower())
  data_clean.extend(sentence)
  sentence = ['<s>'] + sentence + ['</s>']
  #print(sentence)
  #data.append(sentence)
  corpus_bigrams.append(list(nltk.bigrams(sentence)))
for document in test_dataset['text']:
  sentence = tknzr.tokenize(document.lower())
  sentence = ['<s>'] + sentence + ['</s>']
  #print(sentence)
  #test.append(sentence)
  test_bigrams.append(list(nltk.bigrams(sentence)))
#corpus = [token for token in re.split('\W+', corpus.lower()) if token != '']
#corpus = ['<s>'] + corpus + ['</s>']
#print(corpus)
#for dat in data:
#  corpus_bigrams.append(list(nltk.bigrams(dat)))
#for tes in test:
#  test_bigrams.append(list(nltk.bigrams(test)))

#corpus_bigrams

#sentence = 'What is the question?'
#sentence = [token for token in re.split('\W+', sentence.lower()) if token != '']
#sentence = ['<s>'] + sentence + ['</s>']
#print(sentence)
#sent_bigrams = list(nltk.bigrams(sentence))
#sent_bigrams

vocab = Counter(data_clean)
vocab = [token if freq >= 3 else '<UNK>' for token, freq in vocab.most_common()]
#vocab

counts = defaultdict(lambda: defaultdict(lambda: 0))
for bigrams in corpus_bigrams:
  for w1, w2 in bigrams:
    counts[w1][w2] += 1

#counts

# probabilities
probabilities = dict()
test_prob = dict()
i = 0
for bigrams in corpus_bigrams:
  probability = []
  #print(bigrams)
  for w1, w2 in bigrams:
    prob = (1 + counts[w1][w2])/(len(vocab) + sum(counts[w1].values()))
    #print(prob)
    probability.append(prob)
  #print(probability)
  probabilities[i] = (probability)
  i += 1
i = 0
for bigrams in test_bigrams:
  probability = []
  #print(bigrams)
  for w1, w2 in bigrams:
    prob = (1 + counts[w1][w2])/(len(vocab) + sum(counts[w1].values()))
    #print(prob)
    probability.append(prob)
  #print(probability)
  test_prob[i]= (probability)
  i += 1
#probabilities = [(1 + counts[w1][w2])/(len(vocab) + sum(counts[w1].values())) for w1, w2 in corpus_bigrams]
#test_prob = [(1 + counts[w1][w2])/(len(vocab) + sum(counts[w1].values())) for w1, w2 in test_bigrams]

#probabilities

print("Part1: ")
N = len(corpus_bigrams)
sum0 = 0
for i in probabilities:
  a = 1.0
  for prob in probabilities[i]:
    N0 = len(probabilities[i])
    a = a * math.pow(prob, -1/N0)
    #np.prod(prob)
  #print(a)
  if a == 0:
    pass
  else:
    sum0 += a
  
  #sum0 += math.pow(np.prod(probability), -1/N)
print(f"The average perplexity for train data tweet is: {sum0/N}")

N_t = len(test_bigrams)
sum1 = 0
for i in test_prob:
  a = 1.0
  for prob in test_prob[i]:
    N1 = len(test_prob[i])
    a = a * math.pow(prob, -1/N1)
  if a == 0:
    pass
  else:
    sum1 += a
print(f"The average perplexity for test data tweet is: {sum1/N_t}")

print("Part2: ")
reverse_counts = defaultdict(lambda: defaultdict(lambda: 0))
for bigrams in corpus_bigrams:
  for w1, w2 in bigrams:
    reverse_counts[w2][w1] += 1

rev_probabilities = dict()
rev_test_prob = dict()
i = 0
for bigrams in corpus_bigrams:
  probability = []
  #print(bigrams)
  for w1, w2 in bigrams:
    prob = (1 + counts[w2][w1])/(len(vocab) + sum(counts[w2].values()))
    #print(prob)
    probability.append(prob)
  #print(probability)
  rev_probabilities[i] = (probability)
  i += 1
i = 0
for bigrams in test_bigrams:
  probability = []
  #print(bigrams)
  for w1, w2 in bigrams:
    prob = (1 + counts[w2][w1])/(len(vocab) + sum(counts[w2].values()))
    #print(prob)
    probability.append(prob)
  #print(probability)
  rev_test_prob[i]= (probability)
  i += 1
#rev_probabilities = [(1 + reverse_counts[w2][w1])/(len(vocab) + sum(reverse_counts[w2].values())) for w1, w2 in corpus_bigrams]
#rev_test_prob = [(1 + reverse_counts[w2][w1])/(len(vocab) + sum(reverse_counts[w2].values())) for w1, w2 in test_bigrams]

lamb = 0
bf_prob = []
train_prob = []
sum3 = 0
lowest = 10000
best_lamb = 10


for c in range(21):
  sum3 = 0
  for i in test_prob:
    a = 1.0
    for j in range(len(test_prob[i])-1):
      N3 = len(test_prob[i])
      p = math.pow((lamb*test_prob[i][j+1] + (1-lamb) * rev_test_prob[i][j]), -1/N3)
      a = a * p
      #np.prod(prob)
    #print(a)
    if a == 0:
      pass
    else:
      #sum3 += math.pow(a, -1/N_t)
      sum3 += a
  if sum3/N_t <= lowest:
    lowest = sum3/N_t
    best_lamb = lamb
  lamb+=0.05
print(f"best lambda is {best_lamb}")

sum4 = 0
for i in probabilities:
    a = 1.0
    for j in range(len(probabilities[i])-1):
      N4 = len(probabilities[i])
      p = math.pow((best_lamb*probabilities[i][j+1] + (1-best_lamb) * rev_probabilities[i][j]), -1/N4)
      a = a * p
      #np.prod(prob)
    #print(a)
    if a == 0:
      pass
    else:
      sum4 += a
      #sum4 += math.pow(a, -1/N)
print(f"Average perplexity for train data tweet is {sum4/N}")
print(f"Average perplexity for test data tweet is {lowest}")